{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7435a052",
   "metadata": {},
   "source": [
    "# Text Prepocessing\n",
    "    tensorflow & keras & nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009054ed",
   "metadata": {},
   "source": [
    " > Tensorflow = 구글에서 제공한 머신러닝 오픈소스 라이버리\n",
    " > <br> Keras = 텐서플로우에 대한 API, Tensorflow에서 케라스를 사용할 수 있음. \n",
    " > <br> nltk = 자연어 처리를 위한 파이썬 패키지\n",
    " > <br> KoNLPy = 한국어 자연어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e35ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.45.0)\n",
      "Requirement already satisfied: setuptools in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d729a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de8bda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: click in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk #자연어 처리를 위한 파이썬 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c50ba32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/heaji/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # 영어 코퍼스를 토큰화히기 위한 도구\n",
    "nltk.__version__\n",
    "nltk.download('punkt') #word_tokenize쓸려면 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492e43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f50bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8aa0787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 8.2 MB/s eta 0:00:01    |████████████                    | 7.3 MB 6.4 MB/s eta 0:00:02     |█████████████████▎              | 10.5 MB 3.1 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from konlpy) (4.8.0)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl (381 kB)\n",
      "\u001b[K     |████████████████████████████████| 381 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from konlpy) (1.20.3)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b0df932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc196745",
   "metadata": {},
   "source": [
    "# Tokenization (토큰화)  - split한다고 생각하면됨\n",
    " > 1) word_tokenize\n",
    " > <br> 2) WordPunctTokenizer\n",
    " > <br> 3) text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63eb9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1191725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2804866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Jone',\n",
       " \"'s\",\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2bbd616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Jone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10580876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"don't\",\n",
       " 'be',\n",
       " 'fooled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'sounding',\n",
       " 'name',\n",
       " 'mr',\n",
       " \"jone's\",\n",
       " 'orphanage',\n",
       " 'is',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'as',\n",
       " 'cheery',\n",
       " 'goes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pastry',\n",
       " 'shop']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed2c1c",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "## 표준 토큰화 - Penn Treebank Tokenization\n",
    " > 1) 하이픈은 붙여줌 <br> 2) doesn' 은 따로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2f2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d37a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_1 = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0fde3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037f3192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Starting',\n",
       " 'a',\n",
       " 'home-based',\n",
       " 'restaurant',\n",
       " 'may',\n",
       " 'be',\n",
       " 'an',\n",
       " 'ideal.',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'food',\n",
       " 'chain',\n",
       " 'or',\n",
       " 'restaurant',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(phrase_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd477f",
   "metadata": {},
   "source": [
    "# Sentence Tokenization \n",
    "## 미국 - sent_tokenize(text)<br> 한국 - kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e2dbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94ef85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "804c043e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['His barber kept his word.',\n",
       " 'But keeping such a huge secret to himself was driving him crazy.',\n",
       " 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n",
       " 'He dug a hole in the midst of some reeds.',\n",
       " 'He looked about, to make sure no one was near.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407dd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92c358ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘.',\n",
       " '그 후 점심 먹으러 가자.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0e9c321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.4.2.tar.gz (42.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.4 MB 10.1 MB/s eta 0:00:01     |████████████████████████████▉   | 38.2 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[K     |████████████████████████████████| 175 kB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from kss) (2021.11.2)\n",
      "Requirement already satisfied: more_itertools in /Users/heaji/opt/anaconda3/lib/python3.9/site-packages (from kss) (8.12.0)\n",
      "Building wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-3.4.2-py3-none-any.whl size=42448069 sha256=ef38ad2db5a1ffe21944cfabe368ef933aae4c82eaede0ca8427b57d7352ed7d\n",
      "  Stored in directory: /Users/heaji/Library/Caches/pip/wheels/2e/2c/89/2e2c0dfb045fc04987e749eac2c8d97f0075f9d3b299390c79\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=fa3e02b93558e251a2d189f96b2b0c01e5ff401e3218c3d64abc1b457abc1861\n",
      "  Stored in directory: /Users/heaji/Library/Caches/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.7.0 kss-3.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e01a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970f3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f44da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kss.split_sentences(text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89490b",
   "metadata": {},
   "source": [
    "# Part of Speech\n",
    "    미국 pos_tag\n",
    "    한국 konlpy.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00276b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa788e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_4 = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3829bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = word_tokenize(text_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bffb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_1)\n",
    "pos_tag(tokenized_1) #각 word 별로 part of speech "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab75fe",
   "metadata": {},
   "source": [
    "# Cleaning<br> \n",
    "***for example, len 2 인 경우에는 제거하더라도 영어에서는 크게 의미를 갖지 못하는 단어를 줄이는 효과***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae0cf8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f782fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I was wondering if anyone out there could enlighten me on this car.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfdb3f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "167f188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter = re.compile(r'\\w*\\b\\w{1,2}\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12bcc648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering  anyone out there could enlighten   this car.\n"
     ]
    }
   ],
   "source": [
    "print(shorter.sub('',text)) #읽는데는 아무런 지장이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdfac8",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d836247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/heaji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/heaji/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e15fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d71e61ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea3a7208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('ate', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ba5719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"i ate a lot, and i am not healthy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "295d6070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i ate a lot, and i am not healthy.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20f092",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "    PorterStemmer\n",
    "    LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "006e5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f032676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c0a0e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi',\n",
       " 'wa',\n",
       " 'not',\n",
       " 'the',\n",
       " 'map',\n",
       " 'we',\n",
       " 'found',\n",
       " 'in',\n",
       " 'billi',\n",
       " 'bone',\n",
       " \"'s\",\n",
       " 'chest',\n",
       " ',',\n",
       " 'but',\n",
       " 'an',\n",
       " 'accur',\n",
       " 'copi',\n",
       " ',',\n",
       " 'complet',\n",
       " 'in',\n",
       " 'all',\n",
       " 'thing',\n",
       " '--',\n",
       " 'name',\n",
       " 'and',\n",
       " 'height',\n",
       " 'and',\n",
       " 'sound',\n",
       " '--',\n",
       " 'with',\n",
       " 'the',\n",
       " 'singl',\n",
       " 'except',\n",
       " 'of',\n",
       " 'the',\n",
       " 'red',\n",
       " 'cross',\n",
       " 'and',\n",
       " 'the',\n",
       " 'written',\n",
       " 'note',\n",
       " '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "sent = word_tokenize(sentence)\n",
    "\n",
    "[stemmer.stem(word) for word in sent]\n",
    "\n",
    "#example 2\n",
    "#words = ['formalize','formality','allowance','allowed','having','have']\n",
    "#for word in words:\n",
    "#    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6901f11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allowance'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization 하면 안됨.\n",
    "lemmatizer.lemmatize('allowance','n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae49182",
   "metadata": {},
   "source": [
    "# Stopword \n",
    "    * 이유) 큰 의미가 없기 때문에 분석을 할때 제외해도 된다고 생각함 \n",
    "    stopword를 패키지로 지정해놓고 있음.\n",
    "    100 개 이상의 영어 단어들을 미리 정의해놓음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1744e110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/heaji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1585ab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english')) #다해서 179 개 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "377a21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word 제거\n",
    "ex = \"Family is not an important thing. It's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e048adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = word_tokenize(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4cd5015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.'] \n",
      " ['is', 'not', 'an']\n"
     ]
    }
   ],
   "source": [
    "lst_stopword = list()\n",
    "lst_non = list()\n",
    "\n",
    "for word in tokenizer:\n",
    "    if word not in stop_words:\n",
    "        lst_stopword.append(word)\n",
    "    else:\n",
    "        lst_non.append(word)\n",
    "        \n",
    "print(lst_stopword,\"\\n\",lst_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71416dba",
   "metadata": {},
   "source": [
    "# Regular Expression (RE)\n",
    "    re.compile() - 미리 re 를 컴파일 해놓음\n",
    "    re.search() - 매칭되는게 있는지 확인 \n",
    "    re.findall() - 찾아서, list 로 돌려줌\n",
    "    re.sub() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1dda4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e76898",
   "metadata": {},
   "source": [
    "    . : 1 only\n",
    "    ? : 0 or 1\n",
    "    * : 0 or 0 이상\n",
    "    + : 1 or 1 이상 \n",
    "    ^ : ^\"xx\" 로 시작\n",
    "    {2}: {2} 번 만큼 반복\n",
    "    {2,3}: 2이상 3이하 문자열\n",
    "    {2,}: 2이상\n",
    "    [abc]: a, b, c에 있는거와 매치\n",
    "    [^abc]: abc 제외하고 모두매치\n",
    "    [a-c]: a~c에 있는거와 매치\n",
    "    [0-9]: \"\\d\" 동일, 0~9에 있는거와 매치\n",
    "    [a-z]: a~z에 있는거와 매치\n",
    "    [a-zA-Z]: capitalize & lower 모두 포함\n",
    "    \n",
    "    \n",
    "    '\\d+' 숫자하나 이상\n",
    "    '\\s+' 공백하나 이상\n",
    "    '\\w+' 숫자 or 문자 하나 이상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ad27c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ac'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ac'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n",
      "None\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n",
      "<re.Match object; span=(0, 4), match='abbc'>\n"
     ]
    }
   ],
   "source": [
    "r1=re.compile(\"a.c\")\n",
    "r2=re.compile(\"ab?c\")\n",
    "r3=re.compile(\"ab*c\")\n",
    "r4=re.compile(\"ab+c\")\n",
    "\n",
    "print(r1.search(\"ac\")) #None\n",
    "print(r1.search(\"abc\"))\n",
    "print(r1.search(\"abbc\")) #None\n",
    "print(r2.search(\"ac\"))\n",
    "print(r2.search(\"abc\"))\n",
    "print(r2.search(\"abbc\")) #None\n",
    "print(r3.search(\"ac\"))\n",
    "print(r3.search(\"abc\"))\n",
    "print(r3.search(\"abbc\"))\n",
    "print(r4.search(\"ac\")) #None\n",
    "print(r4.search(\"abc\"))\n",
    "print(r4.search(\"abbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "10be562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "<re.Match object; span=(0, 5), match='abbbc'>\n",
      "without compile <re.Match object; span=(0, 5), match='abbbc'>\n",
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 10), match='abbbbbbbbc'>\n",
      "<re.Match object; span=(0, 6), match='abbbbc'>\n",
      "<re.Match object; span=(0, 1), match='2'>\n",
      "<re.Match object; span=(5, 6), match='f'>\n",
      "<re.Match object; span=(1, 2), match='b'>\n"
     ]
    }
   ],
   "source": [
    "r5 = re.compile(\"^abc\")\n",
    "print(r5.search(\"abcd\"))\n",
    "print(r5.search(\"cdcdabcd\"))\n",
    "\n",
    "r6 = re.compile(\"ab{3}c\")\n",
    "print(r6.search(\"abbbc\"))\n",
    "print(\"without compile\",re.search(\"ab{3}c\", \"abbbc\"))\n",
    "\n",
    "r7 = re.compile(\"ab{1,9}c\")\n",
    "print(r7.search(\"abc\"))\n",
    "print(r7.search(\"abbbbbbbbc\"))\n",
    "\n",
    "r8 = re.compile(\"ab{3,}c\")\n",
    "print(r8.search(\"abbbbc\"))\n",
    "\n",
    "r9 =re.compile(\"[0-9]\")\n",
    "print(r9.search(\"213234234\"))\n",
    "\n",
    "r10 = re.compile(\"[f-h]\")\n",
    "print(r10.search(\"abcdef\"))\n",
    "\n",
    "r11 = re.compile(\"[^a]\")\n",
    "print(r11.search(\"abcdef\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff4c60",
   "metadata": {},
   "source": [
    "## re.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2a41b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과', '딸기', '수박', '메론', '바나나']\n",
      "['사과', '딸기', '수박', '메론', '바나나']\n",
      "['사과', '딸기', '수박', '메론', '바나나']\n",
      "['사과', '딸기', '수박', '메론', '바나나']\n"
     ]
    }
   ],
   "source": [
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "print(re.split(\" \", text))\n",
    "print(text.split(\" \"))\n",
    "text_1 = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "print(re.split(\"\\n\", text_1))\n",
    "print(text_1.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c4374",
   "metadata": {},
   "source": [
    "## re.findall()\n",
    "<br> search와 다르게, 찾아서 list로갖다줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "709f02c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"이름 : 김철수\n",
    "전화번호 : 010 - 1234 - 1234\n",
    "나이 : 30\n",
    "성별 : 남\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405540e",
   "metadata": {},
   "source": [
    "## re.sub() , re.findall()\n",
    "<br> re.sub('xxx', ' ' ,text)\n",
    "<br> re.findall(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0657fce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am 26 years old, and will turn 27 next yr'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i am    26 years old, and will turn 27 next yr\"\n",
    "slice_ = re.split(\"\\s+\",text)\n",
    "re.findall(\"\\d+\",text)\n",
    "\n",
    "re.sub(\"\\s{2,}\",\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "653c5981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"\n",
    "\n",
    "re.split(\"\\s+\",text) #공백을 기준으로 값 구분 \n",
    "re.findall('[\\d]+', text) # 숫자 찾아내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8aca1403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capitalize된거찾기\n",
    "\n",
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8c0f8dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#대문자 몇글자 찾기\n",
    "re.findall(\"[A-Z]{3,}\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab52f05",
   "metadata": {},
   "source": [
    "# RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5cd9a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "badbe618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1e287b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "133cfc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']\n",
      "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']\n"
     ]
    }
   ],
   "source": [
    "#한번 비교해보자! \n",
    "#Split만 하는거면 RegexpTokenizer가 split이랑 다른건 없음.\n",
    "#다만 split은 \" \" 지정해서 나눠주고, 이거는 space2개 이상도 잡아줌.\n",
    "\n",
    "sample1= TreebankWordTokenizer().tokenize(text)\n",
    "sample2= RegexpTokenizer(\"\\s+\", gaps=True).tokenize(text)\n",
    "print(sample1)\n",
    "print(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e043778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
    "print(tokenizer1.tokenize(text))\n",
    "tokenizer2 = RegexpTokenizer(\"\\w+\")\n",
    "print(tokenizer2.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92968873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
